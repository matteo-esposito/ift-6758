{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TN3FCLfgPlW"
   },
   "source": [
    "<center><h1> IFT-6758 : Data Science  </h1></center>\n",
    "<center><h2> Fall - 2020 </h2></center> \n",
    "<center><h3> Homework - 2</h3></center> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UCNCKuFTG6Q"
   },
   "source": [
    "[Notebook](https://colab.research.google.com/drive/1CKUlvdEa1bJLS2_P7UeaPUWSZ_ZDUDSH) due November 06, 2020 at [23.59 EST](https://www.worldtimebuddy.com/?qm=1&lid=6077243&h=6077243&date=2020-11-06&sln=23-24) as **PDF** on [Gradescope](https://www.gradescope.com/courses/179325/assignments/773268)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "5gmTBgy5yqYu"
   },
   "outputs": [],
   "source": [
    "#@title Imports (Run this cell first) { run: \"auto\" }\n",
    "plotting_library = \"matplotlib\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Not mandatory to use\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Uncomment this line below if using seaborn\n",
    "sns.set() \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = 'https://raw.githubusercontent.com/Jhelum-Ch/DataScience_IFT6758/gh-pages/media/{}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdfJi5fyg83K"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyc_U4otFmIE"
   },
   "source": [
    "#### **Q1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncJ1cJZSHuS2"
   },
   "source": [
    "**12 points** = $(1.5 + 2 + 2 + 1.5 + 1.5 + 2 + 1.5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv367uHoajzC"
   },
   "source": [
    "The cell below loads a subset of the California Housing dataset. \n",
    "\n",
    "  (a) Store only the `latitude`, `longitude` and `median_house_value` columns in a dataframe denoted by a variable `features`. Produce a scatter plot of the data points with `longitude` along x-axis, `latitude` along y-axis and the points colored by `median_house_value` i.e. higher the `median_house_value`, darker the data point in the plot.\n",
    "  \n",
    "  (b) Perform a PCA on the subset of the dataframe you created in (a) with only the `latitude` and `longitude` columns. Produce a scatter plot of the transformed data points with the first principal component `PC 1` along x-axis and second principal component `PC 2` along y-axis and the points colored by `median_house_value` just like in (a).\n",
    "\n",
    "  (c) Provide a simple interpretation for what the first principal component `PC 1` could possibly represent in the plot in (b) by comparing it with that in (a). **Justify** your answer.\n",
    "  \n",
    "  (d) Repeat what you did in (b) above by setting the `whiten` parameter as `True` in the `PCA()` constructor and producing the plot. What difference do you observe? What do you think `whiten` does specifically in this problem?\n",
    "\n",
    "  (e) Perform a PCA on the entire dataframe `features` with `whiten` set to `True` and produce a scatter plot of the transformed data points with the first principal component `PC 1` along x-axis and second principal component `PC 2` along y-axis and the points colored by `median_house_value` just like in (a).   \n",
    "\n",
    "  (f) Observe how the color indicating `median_house_value` varies in the plot you produced in (e). Is the variation of `median_house_value` depicted in this plot simpler than what is indicated by all the above plots? Provide an **explanation** for why it is (or) it is not the case.\n",
    "  \n",
    "  (g) The California Department of Housing and Community Development (HCD) releases additional information about the data samples you used  here, by providing an `price_index` tag that can take values `high`, `middle` or `low` based `median_house_value`. If you were to eventually use the principal components you produced in (e), which one(s) among the `PC 1`, `PC 2`, ..,. would you use to classify the data samples into these three categories (`high`, `middle` and `low`)? Concretely **justify** your choice.  \n",
    "\n",
    "**Bonus : (3 points)**\n",
    "> (h) In the plot that you observe in (a), you will remark two major clusters that are the *darkest*. Let us identify the cluster with the higher value of `latitude` as the `SF cluster` and the one with the lower value of `latitude` as the `LA cluster`. Verify programmatically if this clustering is preserved or distorted in the plot in (e). What does this tell you about what is represented by the second principal component `PC 2` produced in (e)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gE_1KiX_WukC"
   },
   "outputs": [],
   "source": [
    "housing = pd.read_csv(path.format('california_housing.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH4rzXQum0w2"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OiE56H3m6xw"
   },
   "source": [
    "#### **Q2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnXE_wOVGMxF"
   },
   "source": [
    "**10 points** = $(1 + 2 + 3 + 1.5 + 1.5 + 2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk7uw3bcFuNr"
   },
   "source": [
    "A bird detection system that is equipped with multiple sensors is deployed in an observation station in an open space and it detects and collects some information about birds that visit the space. For each detected bird, it is able to collect the following information: \n",
    "\n",
    "*   `position` - a value of the form (`x`,`y`,`z`) which indicates the coordinates in a three-dimensional space defined by the LiDAR field of the system. `x` and `y` coordinates are in the range (-500,500) whereas `z` coordinate is in the range (0,150) where 0 indicates the ground level for `z`.\n",
    "*   `sound_level` - a value in decibels (0-120 dB) of the sound made by the detected bird with 0 indicating no audible sound and 120 indicating maximum sound that can be detected. \n",
    "*   `time_of_visit` -  a value indicating number of milliseconds since the 00:00 hours of the day of collection.\n",
    "\n",
    "The system sends out a table with the above three fields periodically to you. You are leading a team of data scientists to explore if the data collected by this system can be used to distinguish the different species of birds visiting the station located in the open space.  \n",
    "\n",
    "(a) You decide that using clustering for initially attempting this problem is a good option. How would you *logically* convince your team of this? \n",
    "\n",
    "(b) Your team is trying to decide between using K-means and hierarchical clustering, both based on an Euclidean distance measure. Propose a list of preprocessing operations to be performed on the before you use any clustering algorithm on the raw dataset. **Justify** why you include each step. \n",
    "\n",
    "(c) An enthusiastic intern in your team selects a subset of *raw* data samples from the dataset, selecting some nocturnal birds that visit all together exactly at midnight everyday and they cannot produce any sound. They all have been observed to consistently sit on the ground along a straight line. He generates the following dendrogram (left) using an agglomerative hierarchical clustering with an appropriate linkage that maximizes intercluster dissimilarity. On the right is a top-view visualization of the arrangement of the birds in the station, based on the dendrogram. The bird B1 has already been placed. Place the birds B2 - B9 on the line with an *appropriate* spacing between them. **Explain** your choice.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<img width=750 src=\"https://i.ibb.co/M2ymGvm/nocturnal-birds.png\" alt=\"Nocturnal Birds\" border=\"0\" /> \n",
    "\n",
    "\n",
    "(d)  You are informed by a group of expert ornitholigists that groups of birds that belong to different species, tend to visit the station in almost equal numbers per species. Based on this information, what type of linkage would you use in the hierarchical clustering algorithm? **Justify** your choice.\n",
    "\n",
    "(e) The group of ornithologists have identified that exactly 10 species of birds visit the station. Does this information help you decide between choosing the K-means and hierarchical clustering algorithms? **Explain** why/why not.\n",
    "\n",
    "(f) Briefly outline any **two** ways in which you can verify if the clustering you have performed has captured the natural grouping that exists among the actual data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvIJjaedhaoS"
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1J6xdnxyadK"
   },
   "source": [
    "#### **Q3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD0AN5vp9wkd"
   },
   "source": [
    "**10 points** = $(1 + 2 + 3 + 1.5 + 1 + 1.5)              $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrJtjKbTHyta"
   },
   "source": [
    "Given below is a pair of plots generated while cross-validating a K-NN model trained on a dataset with various values of K using 12-fold CV and Leave-one-out (LOOCV) methods :                             \n",
    "\n",
    "<img width=750 src=\"https://i.ibb.co/cyY7m6X/cv-graphs.png\" alt=\"Trees\" border=\"0\" /> \n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "(a) What is the motivation behind using cross-validation techniques such as LOOCV and k-fold CV over having a validation set?\n",
    "\n",
    "(b) What could possibly explain the difference in the error curves in the two methods in the plots? \n",
    "\n",
    "(c) Describe how similar (or) different the error curves generated on the same dataset would look like for another independent run of **each of** the methods *12-fold* and *LOOCV*, compared to the plots above. **Explain** the reason for your answer in each case.    \n",
    "\n",
    "(d) Under which circumstances would you would favour using LOOCV over k-fold CV?  \n",
    "\n",
    "(e) Based on the above plots, what is the best value for the hyperparameter K of the model? **Explain why.** \n",
    "\n",
    "(f) What type of hyperparameter search do the above plots illustrate? **Explain**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "id": "jeBkKi3Pngd3"
   },
   "source": [
    "## Inference and Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9F-IwGJp1AB"
   },
   "source": [
    "#### **Q4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2Vt-PXKYMwi"
   },
   "source": [
    "**16 points** = $(1.5 + 1.5 + 1.5 + 1.5 + 2 + 2 + 1.5 + 1.5 + 1.5 + 1.5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhCpCHb505gA"
   },
   "source": [
    "The dataset loaded in the next cell consists of data from a drug trial experiment.  \n",
    "* `subject_type` indicates 0 for if a subject is a *control* and 1 if taking *treatment*. \n",
    "* `daily_dosage` indicates the dosage of the drug in millilitres (mL)\n",
    "* `life_expectancy` show the projected age (year) upto which that the subject is expected to live.\n",
    "\n",
    "Let the field `subject_type` in the dataset correspond to $x_{type}$, `daily_dosage` to $x_{dosage}$ and `life_expectancy` to $y$.\n",
    "\n",
    "  (a) Now, consider the regression : \n",
    "  $$y = \\beta_{dosage} x_{dosage} + \\beta_{type} x_{type} + \\beta_{0} + \\epsilon$$\n",
    "\n",
    "  Write your code to perform this regression and list the coefficient estimates $\\hat{\\beta}_{dosage}$, $\\hat{\\beta}_{type}$ and $\\hat{\\beta}_{0}$ that you obtained by running your code.\n",
    "\n",
    "  (b) Run a bootstrap of the dataset over 500 iterations, and collect the coefficients $\\hat{\\beta}_{dosage}$, $\\hat{\\beta}_{type}$ and $\\hat{\\beta}_{0}$ that you obtain in each iteration. \n",
    "\n",
    "  (c) Use the coefficient estimates that you collected in (b) and estimate the standard errors of all the 3 coefficients $S.E.(\\hat{\\beta}_{dosage})$, $S.E.(\\hat{\\beta}_{type})$ and $S.E.(\\hat{\\beta}_{0})$. \n",
    "  \n",
    "  (d) Plot a histogram to observe the distribution of each of the collected coefficient estimates. What do you observe?\n",
    "\n",
    "  (e) Provide a 95% confidence interval for each of the coefficient estimates. What does this interval mean?\n",
    "\n",
    "  (f) Generate the scatterplot for the points in the dataset with `daily_dosage` on the x-axis, `life_expectancy` on the y-axis and the points colored by the `subject_type` value (separate colors to indicate the types 0 and 1) with the collected 500 bootstrap sampled fits overlaid. To make the plot easier to read, reduce the transparency of the lines.\n",
    "\n",
    "  (g) Based on all the above, **explain** intuitively what is conveyed by the plot you generated in (f).\n",
    "    \n",
    "  (h) Make a scatterplot of the bootstrapped coefficients, $\\left(\\beta_{type}^{\\ast}, \\beta_{dosage}^{\\ast}\\right)$ against one another. **Comment** on the overall distribution of these two coefficients **and** the nature of correlation between them.\n",
    "  \n",
    "  (i) Similar to above, estimate the coefficient estimates $\\hat{\\beta}_{dosage}$ and $\\hat{\\beta}_{0}$ and the standard error of the dosage coefficient $S.E.(\\hat{\\beta}_{dosage})$ by bootstrapping the dataset over 500 iterations and fitting the dataset in the regression : \n",
    "  $$y = \\beta_{dosage} x_{dosage} + \\beta_{0} + \\epsilon$$\n",
    "  \n",
    "  (j) Comparing the results in (i) and (c), what can you comment about the relationship of `daily_dosage` and `subject_type` with `life_expectancy`? \n",
    "\n",
    "**Bonus : (3 points)**\n",
    "> (k) In a bootstrap of the dataset over 500 iterations similar to the above, perform an independent t-test with an $\\alpha=0.05$ significance level, using `scipy.stats.ttest_ind` on the `life_expectancy` of these two groups . Collect the p-values and plot the p-value histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXi8VEBPjRJP"
   },
   "outputs": [],
   "source": [
    "trials = pd.read_csv(path.format('drug-trials.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbFa0M57qKML"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyHAWnxmpycJ"
   },
   "source": [
    "#### **Q5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DemYB5AgxQwY"
   },
   "source": [
    "**4 points** = $(1.5 + 2.5)$                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "id": "8GICxsU1ngd1"
   },
   "source": [
    "Given below is a cell that loads a dataset that contains features representing the body measurements of certain types of sharks in various regions in Canada.\n",
    "\n",
    "Using programming, perform analyses using the following methods to identify the outlier samples in the dataset : \n",
    "\n",
    "  (a) For each *feature* among `body_length`, `fin_length` and `tail_length` in the dataset, use a box plot to visualize the feature values (along y-axis) grouped by `region` feature (show on x-axis). Identify the `region` and `feature` (other than `region`) that shows the highest number of outliers. \n",
    "\n",
    "  (b) For the `region` and the `feature` you chose in (a), use the Q3-Q1 Inter-Quartile Range (IQR) to identify and list the rows of the outliers from the dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYaKzsMi38DE"
   },
   "outputs": [],
   "source": [
    "sharks = pd.read_csv(path.format('ca-sharks.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VtpAIK5lsXG"
   },
   "source": [
    "#### **Q6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ratY1dnslsXI"
   },
   "source": [
    "**8 points** = $(1 + 3 + 2 + 2 )$                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "id": "Nwn8YCMAlsXJ"
   },
   "source": [
    "For the questions below, answer briefly by inspecting the dataset below. (There is no need to use any programming) :\n",
    "\n",
    "This is a representative subset of a collected dataset with information about used buses across three Canadian cities. A model needs to be fit to predict the selling price `Price($)` of a bus. `--` indicates that the information is not available. \n",
    "\n",
    "ID|City|Province|Vehicle model|Category|Mileage(kmpl)|Certification|Year|Num_Damages|Price($) \n",
    "--|--|--|--|--|--|--|--|--|--|\n",
    "1|Montréal|QC|CX-120|School|2.5|Certified|2018|3|44900\n",
    "2|Vancouver|BC|AL-100|Tourism|5|Not certified|2016|--|22380\n",
    "3|Toronto|ON|WS-978|Tourism|--|Certified|--|--|30000\n",
    "4|Vancouver|BC|RR8|Factory|1.5|Certified|2012|5|10500\n",
    "5|Toronto|ON|GH-50|Factory|4.5|Not certified|2015|--|12000\n",
    "6|Montréal|QC|--|School|--|Not certified|2010|--|8000\n",
    "\n",
    "\n",
    "(a) List the features that you would remove from the dataset before using it for model fitting. Give valid **reasons** for your answer.\n",
    "\n",
    "(b) List the features that need to be encoded in this dataset and outline which encoding schemes your would use in each case. Give valid **reasons** for your answer.\n",
    "\n",
    "(c) What type of an imputation scheme would make sense for the missing values in each of the fields `Year` and `Mileage`? Give valid **reasons** for your answer.\n",
    "\n",
    "(d) You propose to use the simplest sparsity-based method to select the best features among those given in the dataset. In just two lines, describe the high-level procedure to do this."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IFT-6758 HW-2-(En).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mila3.7",
   "language": "python",
   "name": "mila3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
